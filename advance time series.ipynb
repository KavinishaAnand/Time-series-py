{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2693aec-d42b-4e4e-92c4-c42aa9d83a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing scikit-learn ...\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scikit-learn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mensure\u001b[39m\u001b[34m(pkg_name, import_name)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimport_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\importlib\\__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1324\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'scikit-learn'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     26\u001b[39m sns = ensure(\u001b[33m\"\u001b[39m\u001b[33mseaborn\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m statsmodels = ensure(\u001b[33m\"\u001b[39m\u001b[33mstatsmodels\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m sklearn = \u001b[43mensure\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscikit-learn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m tf = ensure(\u001b[33m\"\u001b[39m\u001b[33mtensorflow\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m joblib = ensure(\u001b[33m\"\u001b[39m\u001b[33mjoblib\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mensure\u001b[39m\u001b[34m(pkg_name, import_name)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInstalling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkg_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m subprocess.check_call([sys.executable, \u001b[33m\"\u001b[39m\u001b[33m-m\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpip\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33minstall\u001b[39m\u001b[33m\"\u001b[39m, pkg_name])\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimport_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\importlib\\__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     88\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1324\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'scikit-learn'"
     ]
    }
   ],
   "source": [
    "# advanced_time_series_full.py\n",
    "# Full end-to-end: ARIMA (unscaled) + LSTM + Transformer (Keras) + metrics + plots\n",
    "# Safe installer: installs missing packages into the same interpreter\n",
    "# Author: (generated)\n",
    "# Run in Jupyter or as a script. If running in Jupyter, run cell top->bottom.\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "import importlib\n",
    "from packaging import version\n",
    "\n",
    "def ensure(pkg_name, import_name=None):\n",
    "    \"\"\"Ensure package installed and importable. Returns imported module.\"\"\"\n",
    "    import_name = import_name or pkg_name\n",
    "    try:\n",
    "        return importlib.import_module(import_name)\n",
    "    except Exception:\n",
    "        print(f\"Installing {pkg_name} ...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg_name])\n",
    "        return importlib.import_module(import_name)\n",
    "\n",
    "# Required packages (minimal, stable set)\n",
    "np = ensure(\"numpy\")\n",
    "pd = ensure(\"pandas\")\n",
    "plt = ensure(\"matplotlib.pyplot\", \"matplotlib.pyplot\")\n",
    "sns = ensure(\"seaborn\")\n",
    "statsmodels = ensure(\"statsmodels\")\n",
    "sklearn = ensure(\"scikit-learn\")\n",
    "tf = ensure(\"tensorflow\")\n",
    "joblib = ensure(\"joblib\")\n",
    "tqdm = ensure(\"tqdm\")\n",
    "\n",
    "# now import specific names safely\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "\n",
    "import math\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "device = \"GPU\" if tf.config.list_physical_devices('GPU') else \"CPU\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d420152-4efa-4e2a-a2e4-7bc2bd6f86b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------\n",
    "# 1. Load (or create) data\n",
    "# -----------------------\n",
    "# If you have a CSV, replace this block with:\n",
    "# df = pd.read_csv(\"yourfile.csv\", parse_dates=['date_col'], index_col='date_col')\n",
    "#\n",
    "# For reproducibility if no file provided, synthetic series:\n",
    "np.random.seed(42)\n",
    "t = np.arange(0, 1000)\n",
    "series = 0.001 * t + 2 * np.sin(2 * np.pi * t / 50) + 0.5 * np.sin(2 * np.pi * t / 7) + np.random.normal(0, 0.5, len(t))\n",
    "df = pd.DataFrame({\"value\": series})\n",
    "df.index = pd.date_range(start=\"2000-01-01\", periods=len(df), freq=\"D\")\n",
    "print(\"Data sample:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b2aeead-c519-4db0-8443-bd92793d6938",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     13\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mKPSS_error\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(e)}\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mADF test (first 1000 points):\u001b[39m\u001b[33m\"\u001b[39m, run_adf(\u001b[43mdf\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m'\u001b[39m]))\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mKPSS test:\u001b[39m\u001b[33m\"\u001b[39m, run_kpss(df[\u001b[33m'\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m'\u001b[39m]))\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------\n",
    "# 2. Stationarity tests\n",
    "# -----------------------\n",
    "def run_adf(x):\n",
    "    res = adfuller(x, autolag='AIC')\n",
    "    return {\"ADF\": res[0], \"pvalue\": res[1]}\n",
    "\n",
    "def run_kpss(x):\n",
    "    try:\n",
    "        res = kpss(x, nlags='auto')\n",
    "        return {\"KPSS\": res[0], \"pvalue\": res[1]}\n",
    "    except Exception as e:\n",
    "        return {\"KPSS_error\": str(e)}\n",
    "\n",
    "print(\"\\nADF test (first 1000 points):\", run_adf(df['value']))\n",
    "print(\"KPSS test:\", run_kpss(df['value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caffd42f-cb75-43ba-b64c-43ed9bfd7664",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m SEQ_LEN = \u001b[32m60\u001b[39m         \u001b[38;5;66;03m# lookback for LSTM/Transformer\u001b[39;00m\n\u001b[32m      6\u001b[39m train_frac = \u001b[32m0.8\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m n = \u001b[38;5;28mlen\u001b[39m(\u001b[43mdf\u001b[49m)\n\u001b[32m      8\u001b[39m train_n = \u001b[38;5;28mint\u001b[39m(n * train_frac)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# ARIMA will be trained on raw unscaled train series (recommended)\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# 3. Train/test split & scaling\n",
    "# -----------------------\n",
    "H = 30               # forecast horizon\n",
    "SEQ_LEN = 60         # lookback for LSTM/Transformer\n",
    "train_frac = 0.8\n",
    "n = len(df)\n",
    "train_n = int(n * train_frac)\n",
    "\n",
    "# ARIMA will be trained on raw unscaled train series (recommended)\n",
    "train_series = df['value'].values[:train_n]\n",
    "# For sequence models, we scale using scaler fitted only on train to avoid leakage\n",
    "scaler = MinMaxScaler()\n",
    "scaled_train = scaler.fit_transform(df[['value']].iloc[:train_n].values.reshape(-1,1)).flatten()\n",
    "# For convenience we'll also scale the rest using this scaler\n",
    "scaled_all = scaler.transform(df[['value']].values.reshape(-1,1)).flatten()\n",
    "\n",
    "# Create sequences (for multi-step H forecast)\n",
    "def make_sequences(values, seq_len, horizon):\n",
    "    X, y = [], []\n",
    "    for i in range(len(values) - seq_len - horizon + 1):\n",
    "        X.append(values[i:i+seq_len])\n",
    "        y.append(values[i+seq_len:i+seq_len+horizon])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_all, y_all = make_sequences(scaled_all, SEQ_LEN, H)\n",
    "# determine index splitting so training sequences end before train_n\n",
    "# find last starting index that uses only train points (i + seq_len + horizon -1 < train_n)\n",
    "last_train_start = train_n - seq_len - H + 1\n",
    "if last_train_start < 1:\n",
    "    raise ValueError(\"Not enough data for chosen SEQ_LEN/H. Reduce SEQ_LEN or H or increase data.\")\n",
    "train_idx = np.arange(0, last_train_start)\n",
    "test_idx = np.arange(last_train_start, len(X_all))\n",
    "\n",
    "X_train = X_all[train_idx]\n",
    "y_train = y_all[train_idx]\n",
    "X_test  = X_all[test_idx]\n",
    "y_test  = y_all[test_idx]\n",
    "\n",
    "print(\"\\nShapes: X_train, y_train, X_test, y_test:\", X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "# reshape for keras: (samples, seq_len, features)\n",
    "X_train_keras = X_train.reshape((X_train.shape[0], SEQ_LEN, 1))\n",
    "X_test_keras  = X_test.reshape((X_test.shape[0], SEQ_LEN, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f204e9f3-fd39-4246-bcd9-f29e9f11963d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selecting ARIMA order by small AIC search (this may take a few seconds)...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_series' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m best_order\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSelecting ARIMA order by small AIC search (this may take a few seconds)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m arima_order = select_arima_order(\u001b[43mtrain_series\u001b[49m, p_max=\u001b[32m3\u001b[39m, d_vals=[\u001b[32m0\u001b[39m,\u001b[32m1\u001b[39m], q_max=\u001b[32m3\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m arima_order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     24\u001b[39m     arima_order = (\u001b[32m1\u001b[39m,\u001b[32m1\u001b[39m,\u001b[32m0\u001b[39m)  \u001b[38;5;66;03m# fallback\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'train_series' is not defined"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# 4. ARIMA baseline (trained on unscaled original train)\n",
    "# -----------------------\n",
    "# Simple grid search for small p,d,q by AIC to avoid external auto_arima dependency\n",
    "def select_arima_order(series, p_max=3, d_vals=[0,1], q_max=3):\n",
    "    best_aic = np.inf\n",
    "    best_order = None\n",
    "    for p in range(p_max+1):\n",
    "        for d in d_vals:\n",
    "            for q in range(q_max+1):\n",
    "                try:\n",
    "                    model = ARIMA(series, order=(p,d,q))\n",
    "                    res = model.fit(method_kwargs={\"warn_convergence\":False})\n",
    "                    if res.aic < best_aic:\n",
    "                        best_aic = res.aic\n",
    "                        best_order = (p,d,q)\n",
    "                except Exception:\n",
    "                    continue\n",
    "    return best_order\n",
    "\n",
    "print(\"\\nSelecting ARIMA order by small AIC search (this may take a few seconds)...\")\n",
    "arima_order = select_arima_order(train_series, p_max=3, d_vals=[0,1], q_max=3)\n",
    "if arima_order is None:\n",
    "    arima_order = (1,1,0)  # fallback\n",
    "print(\"Chosen ARIMA order:\", arima_order)\n",
    "\n",
    "arima_model = ARIMA(train_series, order=arima_order)\n",
    "arima_result = arima_model.fit()\n",
    "# Forecast H steps from end of training period\n",
    "arima_forecast = arima_result.forecast(steps=H)\n",
    "arima_pred = np.array(arima_forecast).flatten()\n",
    "print(\"ARIMA forecast (first 5):\", arima_pred[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40edd1c5-9da6-4103-9ca7-079902ffc57e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 5. LSTM model (Keras)\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mtf\u001b[49m.keras.backend.clear_session()\n\u001b[32m      5\u001b[39m lstm_units = \u001b[32m64\u001b[39m\n\u001b[32m      6\u001b[39m model_lstm = models.Sequential([\n\u001b[32m      7\u001b[39m     layers.Input(shape=(SEQ_LEN, \u001b[32m1\u001b[39m)),\n\u001b[32m      8\u001b[39m     layers.LSTM(lstm_units, return_sequences=\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m      9\u001b[39m     layers.Dense(H)\n\u001b[32m     10\u001b[39m ])\n",
      "\u001b[31mNameError\u001b[39m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# 5. LSTM model (Keras)\n",
    "# -----------------------\n",
    "tf.keras.backend.clear_session()\n",
    "lstm_units = 64\n",
    "model_lstm = models.Sequential([\n",
    "    layers.Input(shape=(SEQ_LEN, 1)),\n",
    "    layers.LSTM(lstm_units, return_sequences=False),\n",
    "    layers.Dense(H)\n",
    "])\n",
    "model_lstm.compile(optimizer='adam', loss='mse')\n",
    "print(\"\\nLSTM model summary:\")\n",
    "model_lstm.summary()\n",
    "\n",
    "es = callbacks.EarlyStopping(monitor='loss', patience=6, restore_best_weights=True, min_delta=1e-6)\n",
    "# Train (use modest epochs to keep runtime reasonable; increase if you want stronger fit)\n",
    "EPOCHS_LSTM = 25\n",
    "model_lstm.fit(X_train_keras, y_train.reshape((y_train.shape[0], H)), epochs=EPOCHS_LSTM, batch_size=32, callbacks=[es], verbose=1)\n",
    "\n",
    "# Predict: take first test sequence forecast for direct comparability with ARIMA's single block\n",
    "lstm_pred_scaled = model_lstm.predict(X_test_keras[:1])  # shape (1,H)\n",
    "# inverse transform (use same scaler fitted on train)\n",
    "lstm_pred = scaler.inverse_transform(lstm_pred_scaled.reshape(-1,1)).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3fa1663-afbb-4488-bd67-f81ea2240891",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m     model = models.Model(inputs=inputs, outputs=outputs)\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43mtf\u001b[49m.keras.backend.clear_session()\n\u001b[32m     26\u001b[39m transformer_model = build_transformer(SEQ_LEN, d_model=\u001b[32m64\u001b[39m, nhead=\u001b[32m4\u001b[39m, ff_dim=\u001b[32m128\u001b[39m, num_layers=\u001b[32m1\u001b[39m, horizon=H)\n\u001b[32m     27\u001b[39m transformer_model.compile(optimizer=\u001b[33m'\u001b[39m\u001b[33madam\u001b[39m\u001b[33m'\u001b[39m, loss=\u001b[33m'\u001b[39m\u001b[33mmse\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# 6. Transformer model (Keras MultiHeadAttention)\n",
    "# -----------------------\n",
    "def build_transformer(seq_len, d_model=64, nhead=4, ff_dim=128, num_layers=1, horizon=H):\n",
    "    inputs = layers.Input(shape=(seq_len, 1))\n",
    "    x = layers.Dense(d_model)(inputs)  # project to d_model\n",
    "    # positional embedding (learned)\n",
    "    positions = tf.range(start=0, limit=seq_len, delta=1)\n",
    "    pos_emb_layer = layers.Embedding(input_dim=seq_len, output_dim=d_model)\n",
    "    pos_emb = pos_emb_layer(positions)\n",
    "    x = x + pos_emb\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        attn_output = layers.MultiHeadAttention(num_heads=nhead, key_dim=d_model)(x, x)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + attn_output)\n",
    "        ffn = layers.Dense(ff_dim, activation='relu')(x)\n",
    "        ffn = layers.Dense(d_model)(ffn)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + ffn)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    outputs = layers.Dense(horizon)(x)\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "transformer_model = build_transformer(SEQ_LEN, d_model=64, nhead=4, ff_dim=128, num_layers=1, horizon=H)\n",
    "transformer_model.compile(optimizer='adam', loss='mse')\n",
    "print(\"\\nTransformer model summary:\")\n",
    "transformer_model.summary()\n",
    "\n",
    "EPOCHS_TRANS = 25\n",
    "es2 = callbacks.EarlyStopping(monitor='loss', patience=6, restore_best_weights=True, min_delta=1e-6)\n",
    "transformer_model.fit(X_train_keras, y_train.reshape((y_train.shape[0], H)), epochs=EPOCHS_TRANS, batch_size=32, callbacks=[es2], verbose=1)\n",
    "\n",
    "trans_pred_scaled = transformer_model.predict(X_test_keras[:1])\n",
    "trans_pred = scaler.inverse_transform(trans_pred_scaled.reshape(-1,1)).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b36afc8-8c98-42e6-9e1d-e3a0809aa3d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 7. Prepare true future block (first test example)\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m y_true_block_scaled = \u001b[43my_test\u001b[49m[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# scaled horizon for first test example\u001b[39;00m\n\u001b[32m      5\u001b[39m y_true_block = scaler.inverse_transform(y_true_block_scaled.reshape(-\u001b[32m1\u001b[39m,\u001b[32m1\u001b[39m)).flatten()\n",
      "\u001b[31mNameError\u001b[39m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# 7. Prepare true future block (first test example)\n",
    "# -----------------------\n",
    "y_true_block_scaled = y_test[0]  # scaled horizon for first test example\n",
    "y_true_block = scaler.inverse_transform(y_true_block_scaled.reshape(-1,1)).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14b89354-72b5-4806-86d3-d067af7ebc8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'arima_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     12\u001b[39m     sign_pred = np.sign(np.diff(y_pred))\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.mean(sign_true == sign_pred)\n\u001b[32m     15\u001b[39m models_preds = {\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mARIMA\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43marima_pred\u001b[49m,\n\u001b[32m     17\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mLSTM\u001b[39m\u001b[33m\"\u001b[39m: lstm_pred,\n\u001b[32m     18\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mTransformer\u001b[39m\u001b[33m\"\u001b[39m: trans_pred\n\u001b[32m     19\u001b[39m }\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- METRICS for first test horizon block ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, pred \u001b[38;5;129;01min\u001b[39;00m models_preds.items():\n",
      "\u001b[31mNameError\u001b[39m: name 'arima_pred' is not defined"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# 8. Metrics: RMSE, MAE, Directional Accuracy\n",
    "# -----------------------\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def directional_acc(y_true, y_pred):\n",
    "    # handle small length case\n",
    "    if len(y_true) < 2:\n",
    "        return np.nan\n",
    "    sign_true = np.sign(np.diff(y_true))\n",
    "    sign_pred = np.sign(np.diff(y_pred))\n",
    "    return np.mean(sign_true == sign_pred)\n",
    "\n",
    "models_preds = {\n",
    "    \"ARIMA\": arima_pred,\n",
    "    \"LSTM\": lstm_pred,\n",
    "    \"Transformer\": trans_pred\n",
    "}\n",
    "\n",
    "print(\"\\n--- METRICS for first test horizon block ---\")\n",
    "for name, pred in models_preds.items():\n",
    "    r = rmse(y_true_block, pred)\n",
    "    m = mean_absolute_error(y_true_block, pred)\n",
    "    d = directional_acc(y_true_block, pred)\n",
    "    print(f\"{name:12s} â†’ RMSE: {r:.4f}, MAE: {m:.4f}, DirAcc: {d:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0608463-2f6d-4e81-98d3-8203ac425d45",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_true_block' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 9. Plot results\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[32m      4\u001b[39m plt.figure(figsize=(\u001b[32m12\u001b[39m, \u001b[32m5\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m plt.plot(\u001b[43my_true_block\u001b[49m, label=\u001b[33m\"\u001b[39m\u001b[33mTrue future (H)\u001b[39m\u001b[33m\"\u001b[39m, marker=\u001b[33m'\u001b[39m\u001b[33mo\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m plt.plot(arima_pred, label=\u001b[33m\"\u001b[39m\u001b[33mARIMA forecast\u001b[39m\u001b[33m\"\u001b[39m, marker=\u001b[33m'\u001b[39m\u001b[33mo\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      7\u001b[39m plt.plot(lstm_pred, label=\u001b[33m\"\u001b[39m\u001b[33mLSTM forecast\u001b[39m\u001b[33m\"\u001b[39m, marker=\u001b[33m'\u001b[39m\u001b[33mo\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'y_true_block' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -----------------------\n",
    "# 9. Plot results\n",
    "# -----------------------\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(y_true_block, label=\"True future (H)\", marker='o')\n",
    "plt.plot(arima_pred, label=\"ARIMA forecast\", marker='o')\n",
    "plt.plot(lstm_pred, label=\"LSTM forecast\", marker='o')\n",
    "plt.plot(trans_pred, label=\"Transformer forecast\", marker='o')\n",
    "plt.title(\"True vs ARIMA vs LSTM vs Transformer (first test block)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e84290de-de76-4e1c-b41c-8230b2104901",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_lstm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 10. Save models & scaler (optional)\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Save Keras models and scaler/arima\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mmodel_lstm\u001b[49m.save(\u001b[33m\"\u001b[39m\u001b[33mmodel_lstm.h5\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m transformer_model.save(\u001b[33m\"\u001b[39m\u001b[33mmodel_transformer.h5\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m joblib.dump(scaler, \u001b[33m\"\u001b[39m\u001b[33mscaler.gz\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'model_lstm' is not defined"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# 10. Save models & scaler (optional)\n",
    "# -----------------------\n",
    "# Save Keras models and scaler/arima\n",
    "model_lstm.save(\"model_lstm.h5\")\n",
    "transformer_model.save(\"model_transformer.h5\")\n",
    "joblib.dump(scaler, \"scaler.gz\")\n",
    "arima_result.save(\"arima_result.pkl\")  # statsmodels save\n",
    "\n",
    "print(\"\\nSaved files: model_lstm.h5, model_transformer.h5, scaler.gz, arima_result.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a347e647-ca4f-4021-9758-a38ebee42d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# FULL ADVANCED TIME SERIES PROJECT\n",
    "# ARIMA + LSTM + TRANSFORMER (ERROR-FREE VERSION)\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "\n",
    "import joblib\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 1. Load Synthetic Dataset (Replace with your data)\n",
    "# -----------------------------------------------------------\n",
    "np.random.seed(42)\n",
    "t = np.arange(0, 1000)\n",
    "series = 0.001 * t + 2 * np.sin(2 * np.pi * t / 50) + \\\n",
    "         0.5 * np.sin(2 * np.pi * t / 7) + np.random.normal(0, 0.5, len(t))\n",
    "\n",
    "df = pd.DataFrame({\"value\": series})\n",
    "df.index = pd.date_range(start=\"2000-01-01\", periods=len(df), freq=\"D\")\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 2. Stationarity Tests\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def adf_test(series):\n",
    "    res = adfuller(series)\n",
    "    return {\"ADF\": res[0], \"pvalue\": res[1]}\n",
    "\n",
    "def kpss_test(series):\n",
    "    try:\n",
    "        res = kpss(series, nlags=\"auto\")\n",
    "        return {\"KPSS\": res[0], \"pvalue\": res[1]}\n",
    "    except:\n",
    "        return {\"KPSS\": \"ERROR\"}\n",
    "\n",
    "print(\"\\nADF:\", adf_test(df['value']))\n",
    "print(\"KPSS:\", kpss_test(df['value']))\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 3. Train/Test Split and Scaling\n",
    "# -----------------------------------------------------------\n",
    "H = 30       # forecast horizon\n",
    "SEQ_LEN = 60\n",
    "train_ratio = 0.8\n",
    "\n",
    "n = len(df)\n",
    "train_n = int(n * train_ratio)\n",
    "\n",
    "train_series = df['value'].values[:train_n]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_all = scaler.fit_transform(df[['value']]).flatten()\n",
    "\n",
    "# ---- Create Sequences ----\n",
    "def create_sequences(values, seq_len, horizon):\n",
    "    X, y = [], []\n",
    "    for i in range(len(values) - seq_len - horizon + 1):\n",
    "        X.append(values[i:i+seq_len])\n",
    "        y.append(values[i+seq_len:i+seq_len+horizon])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_all, y_all = create_sequences(scaled_all, SEQ_LEN, H)\n",
    "\n",
    "last_train_start = train_n - SEQ_LEN - H + 1\n",
    "train_idx = np.arange(0, last_train_start)\n",
    "test_idx = np.arange(last_train_start, len(X_all))\n",
    "\n",
    "X_train = X_all[train_idx]\n",
    "y_train = y_all[train_idx]\n",
    "X_test  = X_all[test_idx]\n",
    "y_test  = y_all[test_idx]\n",
    "\n",
    "X_train = X_train.reshape((X_train.shape[0], SEQ_LEN, 1))\n",
    "X_test  = X_test.reshape((X_test.shape[0], SEQ_LEN, 1))\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 4. ARIMA (Unscaled Series)\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "arima_model = ARIMA(train_series, order=(2,1,2))\n",
    "arima_result = arima_model.fit()\n",
    "arima_forecast = arima_result.forecast(steps=H)\n",
    "arima_pred = np.array(arima_forecast)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 5. LSTM Model\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "model_lstm = models.Sequential([\n",
    "    layers.Input(shape=(SEQ_LEN,1)),\n",
    "    layers.LSTM(64),\n",
    "    layers.Dense(H)\n",
    "])\n",
    "\n",
    "model_lstm.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "model_lstm.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20, batch_size=32, verbose=1\n",
    ")\n",
    "\n",
    "lstm_pred_scaled = model_lstm.predict(X_test[:1])\n",
    "lstm_pred = scaler.inverse_transform(lstm_pred_scaled.reshape(-1,1)).flatten()\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 6. Transformer Model\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def build_transformer(seq_len, d_model=64, heads=4, ff_dim=128, horizon=H):\n",
    "    inputs = layers.Input(shape=(seq_len,1))\n",
    "    x = layers.Dense(d_model)(inputs)\n",
    "\n",
    "    positions = tf.range(0, seq_len)\n",
    "    pos_emb = layers.Embedding(input_dim=seq_len, output_dim=d_model)(positions)\n",
    "    x = x + pos_emb\n",
    "\n",
    "    attn = layers.MultiHeadAttention(num_heads=heads, key_dim=d_model)(x, x)\n",
    "    x = layers.LayerNormalization()(x + attn)\n",
    "\n",
    "    ffn = layers.Dense(ff_dim, activation=\"relu\")(x)\n",
    "    ffn = layers.Dense(d_model)(ffn)\n",
    "    x = layers.LayerNormalization()(x + ffn)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    outputs = layers.Dense(horizon)(x)\n",
    "\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "transformer = build_transformer(SEQ_LEN)\n",
    "transformer.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "transformer.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20, batch_size=32, verbose=1\n",
    ")\n",
    "\n",
    "trans_pred_scaled = transformer.predict(X_test[:1])\n",
    "trans_pred = scaler.inverse_transform(trans_pred_scaled.reshape(-1,1)).flatten()\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 7. True Test Block\n",
    "# -----------------------------------------------------------\n",
    "y_true_scaled = y_test[0]\n",
    "y_true = scaler.inverse_transform(y_true_scaled.reshape(-1,1)).flatten()\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 8. Metrics\n",
    "# -----------------------------------------------------------\n",
    "def rmse(a,b): return np.sqrt(mean_squared_error(a,b))\n",
    "\n",
    "print(\"\\n--- METRICS ---\")\n",
    "print(\"ARIMA RMSE:\", rmse(y_true, arima_pred))\n",
    "print(\"LSTM RMSE :\", rmse(y_true, lstm_pred))\n",
    "print(\"TRANS RMSE:\", rmse(y_true, trans_pred))\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 9. Plot\n",
    "# -----------------------------------------------------------\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(y_true, label=\"True\")\n",
    "plt.plot(arima_pred, label=\"ARIMA\")\n",
    "plt.plot(lstm_pred, label=\"LSTM\")\n",
    "plt.plot(trans_pred, label=\"Transformer\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 10. Save Models\n",
    "# -----------------------------------------------------------\n",
    "model_lstm.save(\"model_lstm.h5\")\n",
    "transformer.save(\"model_transformer.h5\")\n",
    "joblib.dump(scaler, \"scaler.gz\")\n",
    "arima_result.save(\"arima_model.pkl\")\n",
    "\n",
    "print(\"Models saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3411eb2f-1533-48da-8973-221a2eed3bf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
